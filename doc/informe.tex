\documentclass[11pt,a4paper]{article}
\usepackage[latin1]{inputenc}
\usepackage[spanish]{babel}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage{graphicx}
\usepackage{fancyvrb}
\usepackage{color}
\addtolength{\textwidth}{2cm}
\addtolength{\marginparwidth}{-10cm}
\addtolength{\oddsidemargin}{-1cm}

\addtolength{\textwidth}{3cm}
\addtolength{\hoffset}{-1.5cm}
\addtolength{\textheight}{2cm}
\addtolength{\voffset}{-0.5cm}

% Inserts an horizontal line.
\newcommand{\Hrule}{\rule{\linewidth}{0.6mm}}

\fvset{frame=single}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{document}


% Portada.

\begin{titlepage}

\begin{center}

\Hrule \\[0.4cm]
{\Huge \bfseries Proxy HTTP 1.1}\\[0.3cm]
\LARGE{Informe de desarrollo}
\Hrule \\[0.4cm]

\end{center}

\vfill

\begin{center}

\Large{Matías Ezequiel Colotto\\}
\Large{María Eugenia Cura\\}
\Large{Santiago José Samra\\}
\Large{Jorge Ezequiel Scaruli\\}
\vspace{3cm}

\large{Protocolos de Comunicación}\\
\large{2010}

\end{center}

\end{titlepage}


% Documento

\tableofcontents

\clearpage

\section{Introducción}

Se propuso como Trabajo Práctico Especial de la materia Protocolos de
Comunicación el desarrollo de un servidor proxy para el protocolo \textit{HTTP
(Hypertext transfer protocol)} en su versión 1.1, que pueda ser utilizado
para navegar en Internet y que sea compatible con los navegadores
\textit{Mozilla Firefox}, \textit{Google Chrome} e \textit{Internet Explorer}.\\

El objetivo de este informe es dar a conocer las funcionalidades implementadas y
describir el desarrollo de las mismas. Además, se presentan pruebas de carga
realizadas al servidor, así como posibles extensiones de la implementación.

\clearpage


\section{Protocolos desarrollados}

El proyecto se realizó completa y únicamente sobre el protocolo \textit{HTTP}
1.1 especificado por la RFC 2616\footnote{http://tools.ietf.org/html/rfc2616}. Por
lo tanto, no se desarrollaron ni implementaron protocolos propios.\\

El servidor ofrece la posibilidad de ser configurado y monitoreado remotamente
y, aunque esto pudo haber sido implementado mediante protocolos propios, se
decidió que no sea así. Una de las razones de ello fue la posibilidad
de utilizar un protocolo ya existente como \textit{HTTP}, con lo que se evitó la
dificultad de diseñar uno propio teniendo en cuenta las dificultades que
implica. Otra razón fue el hecho de que, al usar \textit{HTTP}, es posible
ofrecer usabilidad web para estos servicios. Finalmente, el hecho de poder
aprovechar las características de parseo de los \textit{requests HTTP}, que
fueron desarrolladas para el funcionamiento del \emph{proxy}, hizo que
implementar el monitoreo y la configuración sea relativamente simple.

\clearpage


\section{Problemas encontrados}

Durante el desarrollo se encontraron gran cantidad de problemas, que debieron
ser solucionados para lograr un producto sólido. Se mencionan a continuación
algunos de ellos:

\subsection{Decisión sobre el uso de \textit{threads} o E/S asincrónica}

En un principio, para ponerse en contacto con las funcionalidades que ofrece
Java para programar servidores concurrentes y manejo de \textit{threads}, se
procedió a implementar pequeños servidores que ofrezcan servicios básicos (como
por ejemplo, \textit{echo}). Estas implementaciones se hicieron utilizando
las dos alternativas analizadas en la materia:

\begin{enumerate}
  \item Teniendo un servidor que, por cada pedido de un cliente, lance un nuevo
  \textit{thread} que lo atienda.
  \item Mediante entrada/salida asincrónica (utilizando el mecanismo
  \textit{select}), en el cual los pedidos entrantes de los clientes se
  mantienen inactivos hasta que el servidor los tome y los atienda.
\end{enumerate}

Analizando el funcionamiento de cada una de las dos alternativas, se observó que
la primera tenía un funcionamiento más acorde a lo que se esperaba del servidor
ya que, básicamente, la segunda opción tenía un modo de trabajar basado en un
solo thread; entonces, más allá de soportar que varios pedidos de clientes
se hagan simultáneamente, no era posible que esos pedidos se atendieran al mismo
tiempo.\\

Sin embargo, se pensó en una tercera alternativa que consistía en una
combinación de las dos anteriormente mencionadas. Consistía en que el servidor
conste de tres \textit{threads}:
\begin{itemize}
  \item Uno que espere conexiones entrantes, y las coloque en una lista de
  conexiones a monitorear por el mecanismo \textit{select}.
  \item Uno que se encargue de realizar las operaciones de lectura en las
  conexiones que se sacaban de dicha lista.
  \item Otro que realice las operaciones de escritura.
\end{itemize}
Con esto, se lograba un funcionamiento más eficiente del mecanismo
\textit{select}, porque no era necesario que el servidor esté desocupado para atender una conexión
entrante; era posible que un thread acepte conexiones mientras otro las
atendía. Asimismo, se evitaba el lanzamiento de un nuevo \textit{thread} por
cada conexión, algo que implicaba un gran consumo de memoria y procesamiento.\\

No obstante, la tercera opción no fue llevada a la práctica, debido a que se
intentó programar un servidor \textit{echo} implementándola y se observó que el
funcionamiento ideal implicaba un gran control de sincronización entre los
\textit{threads}, algo que difícilmente fue logrado en el servidor \textit{echo}
luego de muchos intentos. Por lo tanto, intentar aplicarlo en el proxy llevaría
a grandes complicaciones.\\

Finalmente, se decidió implementar una variante de la primera opción mencionada,
en la cual en vez de disparar un nuevo \textit{thread} por cada pedido de un
cliente, se tome uno entre varios disponibles de un \textit{pool}. De esta
manera, se limitó la cantidad de \textit{threads} existentes al mismo tiempo en
el servidor. Aunque esta limitación implica una menor cantidad de
\textit{threads} funcionando simultáneamente en el servidor, hace que el consumo
de memoria sea moderado y que no se requiera tanta capacidad de procesamiento.
De todos modos, la cantidad de \textit{threads} presentes en el \textit{pool}
puede ser configurada mediante el archivo de configuración del proxy,
con lo cual estas limitaciones pueden aplicarse en función de las
características de la máquina que sirva el proxy y así aprovecharlas para un
funcionamiento óptimo.

\clearpage


\subsection{Interpretación y parseo de mensajes \textit{HTTP}}

Luego de la decisión tomada acerca de cómo atender clientes, se llevó a cabo el
parseo de mensajes \textit{HTTP}.\\

Antes de diseñar los \textit{parsers}, se pensó una jerarquía de clases que
modele los paquetes \textit{HTTP}\footnote{Puede encontrarse en el paquete
\textbf{org.cssc.prototpe.http}}. A grandes rasgos, consistió en una clase
padre \verb+HttpPacket+ de la cual heredan \verb+HttpRequest+ y
\verb+HttpResponse+, y que contienen un método
(\verb+GET+, \verb+POST+ o \verb+HEAD+, si se trata de un
\textit{request}), un \textit{status code} (si se trata de un
\textit{response}) y un \textit{header} con entradas campo-valor. En un
principio se pensó en que las clases tengan también un \textit{body}
asociado, ya que tanto los
\textit{requests} como los \textit{responses} podrían ocasionalmente tener un
cuerpo. Sin embargo esto no se realizó, y la razón es mencionada
luego.\\

Utilizando la herramienta
\textit{jFlex}\footnote{http://jflex.de/}, se programaron los parsers que, en
base al \verb+InputStream+ de un \textit{socket}, construyan el
\verb+HttpRequest+ o \verb+HttpResponse+ correspondiente. En este punto fue
cuando se notó que incluir el cuerpo del mensaje dentro de esas clases no era conveniente,
debido a que ello implicaba parsearlo por completo y asociárselo a la clase, y
no se tenían en cuenta los siguiente factores:
\begin{enumerate}
  \item Es conveniente devolverle al cliente la información del cuerpo dividida
  en partes, para que el navegador pueda construir lo que dicha información
  represente a medida que le va llegando, y no que tenga la obligación de
  recibir todo el cuerpo para recién visualizar el contenido.
  \item El mensaje puede tener el campo \verb+Transfer-encoding: chunked+, con
  lo cual, si se parsea todo el contenido del cuerpo y una vez hecho esto se lo
  envía, se pierde la funcionalidad que un paquete \verb+chunked+ ofrece (por
  ejemplo, la generación de páginas dinámicas).
\end{enumerate}
Por lo tanto, se decidió que los parsers tengan métodos para leer el paquete sin
cuerpo, y otros para leer el próximo \verb+chunk+ del cuerpo si el contenido es
\verb+chunked+, o leer los próximos \textit{n} bytes del mismo si no lo es.

\subsection{Compatibilidad con HTTP 1.0}

Si bien el proxy funciona con el protocolo \textit{HTTP} en su versión 1.1, debe
ser compatible hacia atrás con la versión 1.0.\\

En principio no se encontraron grandes dificultades, debido a que se testeaba
con sitios que devolvían \textit{responses} versión 1.1. Sin embargo, al probar
con el proxy transparente utilizado en el ITBA, se descubrió que el
funcionamiento no era del todo correcto.\\

En primera medida, el proxy transparente utilizado en el ITBA suele cerrar las
conexiones sin avisar. Esto generó problemas en el manejador de conexiones, que
suponía que si una respuesta no aparecía con el header \verb+Connection: close+,
no debía cerrar la conexión ya que ésta se mantendría viva (siguiendo el
estándar \textit{HTTP 1.1}).\\

Otro problema que surgió fue el hecho de que el servidor no manejaba respuestas
\emph{chunked}. Se presentó la problemática de que el proxy implementado no
funcionaba del todo bien con sitios que devolvían respuestas \emph{chunked},
pero dichas páginas funcionaban sin problema al testear desde el ITBA. En
relación a esto, luego de testear y leer el RFC, se descubrió que, ante la
ausencia de respuestas \textit{chunked}, se debe leer del socket del
servidor \textit{origin} hasta que se cierre la conexión. Como este caso fue
contemplado, no hubo problemas para que la recepción de estas respuestas sea
correcta.\\

Si bien podrían pensarse como problemas menores, el hecho de que el
comportamiento del proxy en el ITBA difiera del comportamiento del mismo proxy
en otro lugar generó confusión y dificultó la solución de otros problemas, que
por estos motivos no se podían encontrar.\\

\textbf{\color{red}{\large{Ver si queremos agregar algo más sobre el proxy
del ITBA.}}}

\subsection{Conexiones persistentes}

Una de las funcionalidades requeridas para el proxy fue la de implementar
conexiones persistentes; es decir, que no se cree una nueva conexión con el
servidor \textit{origin} cada vez que se hace un \textit{request}, sino mantener
abiertas las conexiones. De esta manera, ante un \textit{request} a un servidor
con una conexión ya abierta, se evita tener que abrir una nueva.\\

En principio, se mantenía abierta una conexión para cada servidor
\textit{origin}. Pero esto generaba un funcionamiento ineficiente, ya que no
podían haber dos conexiones al mismo servidor simultáneamente, con lo cual las conexiones
persistentes reducían la capacidad del proxy. Por lo tanto, se optó por tener,
para un mismo servidor \textit{origin}, $n$ conexiones (donde $n$
es configurable), y tener un máximo de $m$ conexiones abiertas.\\

La implementación realizada se basa en semáforos\footnote{Ver la clase
\textbf{org.cssc.prototpe.PersistentSemaphorizedServerManager}}. Existe primero
un semáforo que se inicializa con $m$ permisos. Al solicitarle una conexión a 
un servidor al \verb+ServerManager+, se pide un permiso de dicho semáforo; al
liberarse una conexión, se libera un permiso del mismo semáforo. De manera
similar, para cada servidor, existe un semáforo inicializado con $n$ permisos;
cuando se desea obtener una conexión para dicho servidor, se solicita un permiso
al semáforo correspondiente a ese servidor, y cuando se libera una conexión
a dicho servidor, se libera un permiso.\\

Para determinar si se debe crear una
nueva conexión o no, lo que se hace es guardarse por cada servidor una cola de
\textit{sockets} abiertos: una vez adquirido el permiso del semáforo
correspondiente al servidor, se fija si esta cola está vacía. Si no lo está,
entonces significa que hay un \textit{socket} abierto a ese servidor, listo para
ser usado; si está vacía, entonces se tiene permiso para crear una nueva
conexión, ya que se pudo entrar al semáforo del servidor. Cuando se libera un
\textit{socket}, si no está cerrado, se agrega a la cola de conexiones 
persistentes para el servidor correspondiente: de esta manera, se reusan los
\textit{sockets}.\\

Como hay un máximo de $m$ conexiones persistentes abiertas al mismo tiempo,
independiente de cuál sea su servidor, se implementó, lógicamente, un mecanismo
para cerrarlas. En las colas de conexiones de cada servidor aparecen solamente
\textit{sockets} que no están en uso. Por lo tanto, cuando se necesita crear un
nuevo \textit{socket}, si se excede la capacidad de conexiones persistentes, se
busca aquel servidor del que no se hayan pedido conexiones hace
más tiempo, y se libera un socket que aparezca en dicha cola (o, en su defecto,
se sigue buscando una cola que tenga conexiones abiertas). Esta conexión seguro
existe debido a la existencia del semáforo con $m$ permisos: este semáforo se
asegura de que, en cualquier momento, haya como máximo $m$ threads hablando con
algún servidor. Como el thread actual no ha obtenido aún una conexión, hay en
ese momento $m - 1$ threads como máximo hablando con algún servidor; como hay
$m$ conexiones ya abiertas, hay al menos una que no está siendo usada.\\

Paralelamente a los problemas de velocidad, aparecieron también otros, 
más serios, entre los cuales se pueden mencionar:

\begin{itemize}
  \item El encargado de mantener conexiones persistentes con los servidores,
  después de un tiempo, dejaba de otorgar conexiones y dejaba a los threads
  bloqueados. Esto se debía a que no se estaba liberando siempre el socket
  correspondiente a la conexión con el servidor.
  \item Las conexiones con los clientes nunca se cerraban, y el proxy
  simplemente se quedaba leyendo de las mismas. Se debió implementar un timeout
  en dicha lectura, y hacer que se cierren las conexiones cuando se supere ese
  timeout.
  \item Se recibían \textit{responses} sucios cuando se reusaba una conexión a un
  servidor. Esto ocurría debido a que no se leía correctamente la respuesta
  anterior, con lo que quedaban bytes pertenecientes a la misma; al intentar
  leer la nueva \textit{response} del siguiente \textit{request}, se leían dichos
  bytes, generando un error.
\end{itemize}

En síntesis, la implementación de conexiones persistentes tanto entre el cliente
y el proxy como entre el proxy y los servidores \textit{origin} fue la principal
causa de problemas en el desarrollo del trabajo práctico, y su solución nunca
fue simple, debido a que por la naturaleza del servidor y la existencia de
threads concurrentes, resultaba complicado reproducir los bugs.

\subsection{Monitoreo}

El diseño implementado no contempló, desde un principio, encapsular a los
\textit{sockets} en clases específicas del proxy. Las conexiones, así como la
entrada/salida, se manejan directamente utilizando la clase \verb+Socket+.
Debido a esto, se presentaron importantes problemas al intentar contabilizar la
cantidad de bytes transferidos. La solución fue hacer subclases de
\verb+InputStream+ y \verb+OutputStream+ que permitan contabilizar la cantidad de bytes
escritos o leídos.\\

Se presentó además un pequeño problema, relacionado con el monitoreo. Cuando se
intentaban contabilizar la cantidad de bloqueos, a veces se contabilizaba más de
uno cuando mediante un navegador se accedía a una página bloqueada. Esto era
debido a que el navegador intentaba, al mismo tiempo que accedía a la página,
buscar el \textit{favicon} de la misma. Inclusive, el navegador reintentaba varias
veces ir a buscar el \textit{favicon}. Debido a esto, se concluyó que en
realidad la implementación era correcta.

\subsection{Configuración remota}

Debido a las características de la implementación, hay ciertos parámetros que no
son configurables remotamente y sólo pueden ser cambiados
directamente desde el XML de configuración y reiniciando el proxy. Entre ellos,
por ejemplo, se encuentra la cantidad de threads que hay en el \textit{pool}, y la
cantidad máxima de conexiones persistentes.\\

\textbf{\color{red}{\large{Agregar info.}}}

\subsection{Servidores y clientes que no hablan correctamente el protocolo}

Se presentaron dificultades al dialogar con servidores o clientes que no hablan
correctamente el protocolo. Por ejemplo, al intentar ver una imagen determinada,
se presentó el problema de que no se podía leer la imagen para cargarla en
memoria y luego rotarla; esto ocurría porque a pesar de que el
\textit{content-type} declarado en la respuesta era \textit{image/jpeg}, el servidor
en realidad estaba devolviendo un XML.\\

Otro problema consistió en que algunos navegadores como \textit{Mozilla Firefox}
envían \textit{headers} no descriptos por el RFC 2616, como
\textit{Proxy-Connection} en vez de \textit{Connection}. Se debió lidiar con estas 
inconsistencias en el proxy.\\

\textbf{\color{red}{\large{Poner cualquier otra vergüenza o burrada que se
manden los que programen clientes o servidores HTTP. Aunque la verdad los
entiendo, porque honestamente los programas estos hacen cualquier cosa y por
ahi con esa verdura los programas funcionan mejor. Bueno, en realidad lo de
image/jpeg que devuelve un xml no lo entiendo.}}}

\subsection{Proxy transparente}

El proxy transparente fue probado en un ambiente Linux, usando
\textit{firewalls}; más precisamente, el comando \verb+iptables+.\\

En un principio, se observó que el proxy, utilizado en forma transparente, no
tenía el mismo comportamiento que siendo configurado en forma manual en los
navgeadores. Sin embargo, luego se aumentó el número de \textit{threads}
disponibles en el \textit{pool} y se pudo ver que el funcionamiento era mucho
mejor.\\

Finalmente, se concluyó que, al no ser configurado manualmente, el navegador
lanza la misma cantidad de conexiones que cuando no tiene un proxy como
intermediario; esto no sucede cuando se le especifica un proxy, ya que realiza
un número limitado de conexiones. Al iniciar muchas conexiones, esta cantidad
eventualmente puede superar a la que hay disponible en el \textit{pool} de
\textit{threads}, provocando que algunos \textit{requests} deban esperar para
hacerse y recibir finalmente un \textit{response}.\\

Luego de esto, se observó que el proxy en modo transparente funcionaba
correctamente.

\clearpage

\section{Pruebas de carga}

Se realizaron pruebas de carga para testear el funcionamiento del proxy
utilizando la herramienta \emph{Apache
JMeter}\footnote{http://jakarta.apache.org/jmeter/}. Todos los testeos se
realizaron utilizando 10 threads, y con el proxy ejecutándose en la misma
computadora que los testeos.\\

En primera medida, se compararon las tres implementaciones que se realizaron de
conexiones persistentes de proxy a \emph{origin server} probando pedidos GET a
www.google.com.ar. Configurando al proxy correctamente para que permita
suficientes conexiones persistentes al servidor, se obtuvo que la implementación
basada en semáforos (\verb+SemaphorizedPersistentServerManager+), que poseía
múltiples conexiones persistentes por servidor, obtenía un rendimiento de requests 
respondidos por segundo de alrededor del 15\% por sobre la implementación que no persistía conexiones
(\verb+SimpleServerManager+);
mientras que ambas fueron claramente superiores a la implementación que solo
poseía una conexión persistente por servidor (\verb+PersistentServerManager+),
como puede verse en la \textbf{Tabla 1}.\\

% Para una imagen
%\begin{center}
% \includegraphics[scale=1]{diagrama.png}\\
% \vspace{0.5cm}
% \small{\textbf{Figura 2.} Diagrama de estados para los subgrupos establecidos
% anteriormente.}
% \vspace{0.5cm}
%\end{center}
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
Implementación & Throughput (requests por seg.)\\
\hline
\hline
PersistentSemaphorizedServerManager & 39,6\\
SimpleServerManager & 34,2\\
PersistentServerManager & 4,2\\
\hline
\end{tabular}\\
\vspace{0.5cm}
\small{\textbf{Tabla 1.} \emph{Throughput} de requests a
http://www.google.com.ar/ por segundo, con 10 threads, para las distintas implementaciones.}
\vspace{0.5cm}
\end{center}

Se sospechó que el overhead de las conexiones persistentes jugaría un papel más
significativo si el costo de la conexión fuera más pesado: para ello, se
realizó un segundo testeo utilizando el método HEAD a http://www.konami.jp/. El
tiempo de latencia promedio para este servidor fue de 352 ms, mientras que para
http://www.google.com.ar/ era de apenas 13 ms. En la \textbf{Tabla 2} se pueden
ver los resultados de esta prueba, que corroboraron las sospechas planteadas.
En particular, cabe resaltar que, aunque la relación entre la implementación
que permite múltiples conexiones persistentes a un mismo servidor y la
implementación que permite una única conexión persistente permaneció constante
entre ambas pruebas, la relación entre el primero y el que crea una nueva
conexión cada vez no se mantuvo; lo que muestra claramente que la performance
del SimpleServerManager se degradó al incrementarse el tiempo de latencia.\\

\begin{center}
\begin{tabular}{|c|c|c|}
\hline
Implementación & Throughput (requests por seg.)\\
\hline
\hline
PersistentSemaphorizedServerManager & 21,5\\
SimpleServerManager & 12,6\\
PersistentServerManager & 2,3\\
\hline
\end{tabular}\\
\vspace{0.5cm}
\small{\textbf{Tabla 2.} \emph{Throughput} de requests a
http://www.konami.jp/ por segundo, con 10 threads, para las distintas
implementaciones.}
\vspace{0.5cm}
\end{center}

Se concluye, entonces, que la implementación de conexiones persistentes en un
cliente \emph{HTTP 1.1} es especialmente importante cuando se habla con
servidores con los cuales el tiempo de latencia es alto.\\

Por otra parte, se probó el funcionamiendo del servidor proxy cuando la cantidad
de conexiones de clientes superaba la cantidad de threads. Los resultados no
fueron alentadores: en la \textbf{Tabla 3} se pueden ver las performances que
lograron los distintos \verb+ServerManager+ con la misma prueba de pedidos GET
a http://www.google.com.ar/ al bajar la cantidad de threads en el pool a 5
(manteniendo la cantidad de threads utilizado para la prueba en 10).
Básicamente, la performance bajó a la mitad. Por lo tanto, es crucial que el
numero de threads en el pool sea lo suficientemente grande como para poder
contener a todos los clientes que fueran a utilizar el proxy.\\

\begin{center}
\begin{tabular}{|c|c|c|}
\hline
Implementación & Throughput (requests por seg.)\\
\hline
\hline
PersistentSemaphorizedServerManager & 20,5\\
SimpleServerManager & 18,5\\
PersistentServerManager & 4,4\\
\hline
\end{tabular}\\
\vspace{0.5cm}
\small{\textbf{Tabla 3.} \emph{Throughput} de requests a
http://www.google.com.ar/ por segundo, con 10 threads, para las distintas
implementaciones, usando un pool de 5 threads para atender a los clientes.}
\vspace{0.5cm}
\end{center}

Basados en pruebas en los navegadores más populares, se determinó que la
cantidad de threads sugerida para que funcione como proxy (en particular
en el caso de que se desempeñe como proxy transparente) debía ser de al menos 70
threads por cada cliente en potencia. En general, si funciona como proxy
configurado, es suficiente una cantidad mucho menor de threads por cliente en
potencia, como ser 10. Esto se debe a que los navegadores, al saber que existe
un proxy, limitan la cantidad de conexiones a un número razonable, debido a que
interpretan que superar ese número no brindaría mejoras en performance; en
cambio, si no conocen de la existencia del proxy, generan una nueva conexión por
cada servidor distinto al que accedan, con lo cual la cantidad de conexiones es
mucho mayor.\\

\Huge{No se si esto deberia ir aca o en otra seccion}\\

Se sugiere, entonces, por ejemplo, que si el servidor proxy funcionara en un
laboratorio de 20 computadoras y el mismo estuviera configurado en los
navegadores, que se utilicen 200 threads para el pool; o bien 1400, si se desea
que actúe como proxy transparente. Es comprensible que esto demande muchos
recursos: en ese caso, se sugiere bajar el timeout de las conexiones
persistentes a los clientes y la cantidad de threads.

\clearpage


% \section{Más funcionalidades}
% 
% A continuación se describen más funcionalidades del proxy, en cuyas
% implementaciones no se tuvieron problemas mayores.
% 
% \textbf{\color{red}{\large{Si alguna de las cosas que está acá trajo
% problemas, tiene que moverse a la sección anterior.}}}
% 
% \subsection{Encadenamiento de proxies}
% 
% El proxy admite funcionar redirigiendo sus requests hacia otro proxy.
% 
% \textbf{\color{red}{\large{Seguir con la descripción de esta funcionalidad.}}}
% 
% \clearpage


\section{Limitaciones}

El proxy desarrollado tiene las siguientes limitaciones:

\begin{itemize}
  \item No soporta \emph{pipelining} de requests al servidor. Por ello, si bien
  se reusan las conexiones a los servidores cuando llegan distintos requests, la
  eficiencia de ello deja que desear.
  \item El número de conexiones simultáneas a un servidor \textit{origin} está
  limitado, debido a la implementación de conexiones persistentes.
  \item No soporta todos los métodos \textit{HTTP}, solo \verb+GET+, \verb+HEAD+
  y \verb+POST+; con lo cual hay ciertas páginas que no funcionan del todo
  bien porque usan otros métodos.
  \item Para realizar filtrado de páginas por \textit{Content-length}, es
  necesario parsear el contenido completo de un paquete. Por lo tanto, no se le
  puede enviar la información de a pedazos al cliente en el caso de que no haya
  un header \emph{content-length}: se debe leer todo el contenido y luego enviar
  la información, con lo cual se pierde en eficiencia.
\end{itemize}

\clearpage


\section{Posibles extensiones}

En esta sección se mencionan algunas posibles extensiones que podrían
realizársele a esta implementación del proxy \textit{HTTP}.

\begin{itemize}
  \item Podría agregarse la funcionalidad de ser usado como servidor caché,
  opción muy utilizada en varios servidores proxy.
  \item Sería útil almacenar estadísticas sobre qué sitios son visitados por
  ciertas direcciones IP destino, y en base a ello modificar las respuestas de
  dichos sitios con información destinada a algún fin en partícular (por
  ejemplo, publicidad). Si se implementara la funcionalidad de servidor caché,
  también sería posible, en base a la estadística, determinar que páginas
  podrían tener más prioridad de almacenamiento en el servidor que otras. Las
  conexiones persistentes también podrían mantenerse por un tiempo determinado
  por dichas probabilidades.
  \item Podrían agregarse prioridades a las IPs destino, con el fin de
  posibilitarle el uso de un mayor ancho de banda a unas u otras, basándose en
  dicha posibilidad. También podrían utilizarse esas medidas para permitir mayor
  o menor cantidad de conexiones simultáneas a servidores \textit{origin} para
  ciertos clientes.
  \item Además de los bloqueos ya existentes, sería posible bloquear el acceso a
  una URI o IP luego de transferida cierta cantidad de bytes entre un cliente y
  un servidor \textit{origin}.
\end{itemize}

\clearpage

\section{Conclusión}

El desarrollo de un proxy \textit{HTTP} fue aportador desde el punto de vista
de la comprensión del funcionamiento del protocolo.\\

Fue muy interesante el hecho de tener la posibilidad de interceptar la información
enviada por un cliente o un servidor, y poder modificarla a gusto. Se observó
que esto puede tener muchas utilidades, ya sea las implementadas o las
mencionadas en las posibles extensiones. Es impactante ver como, configurando un proxy, 
se puede interceptar la información a, o desde, por ejemplo, un determinado rango de IPs,
para de esta forma \"robar\" datos privados que no estuvieran encriptados, 
o utilizarlos con fines publicitarios. Todo esto (mediante un proxy transparente) 
se estaría haciendo de forma que el cliente ni siquiera se entere de que sus datos
estan a disponibilidad absoluta del que ha configurado el proxy.\\

Las principales dificultades encontradas tuvieron que ver con que no todas las
aplicaciones que lo aplican cumplen el estándar existente. Como en el caso del
proxy se tuvo contacto con navegadores, servidores \textit{origin} y otros
servidores proxy, se observaron varias prácticas que iban en contra de lo dicho
en el RFC. Esto es entendible teniendo en cuenta que el protocolo es muy extenso,
y respetar la documentación puede traer algunas deficiencias en cuanto a
\textit{performance}; no obstante, se concluye que la implementación de
aplicaciones que utilicen el protocolo \textit{HTTP} sería más sencilla si los
estándares fueran respetados.\\

\textbf{\color{red}{\large{Agregar algo más si hace falta.}}}

\clearpage

\section{Ejemplos de testeo}

\section{Guía de instalación}

\section{Guía de configuración}

\section{Ejemplos de configuración}

\section{Documento de diseño}

\end{document}